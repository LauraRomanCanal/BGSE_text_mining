{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGSE Text Mining Homework 2\n",
    "## Euan Dowers, Veronika Kyuchukova, and Laura Roman\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "The objective of this exercise is to implement uncollapsed gibbs sampling for fitting an LDA model to state of the union speeches from 1945 onwards, with documents being defined at the paragraph level. \n",
    "\n",
    "First, we need to read in and process the data, as in the first homework set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse as ssp\n",
    "import time\n",
    "import matplotlib\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from numpy.random import dirichlet\n",
    "from collections import Counter\n",
    "from utils import data_processing, get_vocab, make_count\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.9 s, sys: 16 ms, total: 8.92 s\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table(\"HW1/speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data_post1945 = data.loc[data.year >= 1945]\n",
    "%time stemmed, processed_data = data_processing(data_post1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function that implements uncollapsed gibbs sampling on our processed data.\n",
    "This essentially works by repeatedly sampling from the posterior distributions of $Z$, $\\Theta$, and $\\beta$ and updating values using the most recent sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Gibbs_sampling_LDA(stemmed, K, alpha = None, eta = None, m=3, n_samples = 200, burnin = 500, perplexity = False):\n",
    "    '''\n",
    "    Gibbs sampler for LDA model\n",
    "    '''\n",
    "\n",
    "    def Z_class_1(Beta, Theta):\n",
    "        Z = [np.ndarray.tolist( np.argmax( Beta[:,[idx[word] for word in stemmed[i]]] * \\\n",
    "        Theta[i,:].reshape((K, 1)), axis = 0) ) for i in range(Theta.shape[0] )]\n",
    "        return Z\n",
    "\n",
    "    def Beta_sample(eta, Z):\n",
    "        z_s = [z for sublist in Z for z in sublist ]\n",
    "        M = np.zeros(shape=(K,V))\n",
    "        for k in range(K):\n",
    "            words = [s[i] for i in range(len(z_s)) if z_s[i] == k]\n",
    "            counts = Counter(words)\n",
    "            for word in set(words):\n",
    "                M[k,idx[word]] = counts[word]\n",
    "        Beta = [dirichlet(alpha = eta + M[i],size = 1)[0] for i in range(K)]\n",
    "        return np.array(Beta)\n",
    "\n",
    "    def Theta_sample(alpha, Z):\n",
    "        N   = np.zeros(shape=(D,K))\n",
    "        for i in range(D):\n",
    "            counts   = Counter(Z[i])\n",
    "            for j in set(counts.keys()):\n",
    "                N[i,j]  = counts[j]\n",
    "        Theta = [dirichlet(alpha = alpha + N[i],size = 1)[0] for i in range(D)]\n",
    "        return np.array(Theta)\n",
    "\n",
    "    def onehotencode(Z):\n",
    "        '''\n",
    "        Create function to one-hot encode topic allocation\n",
    "        '''\n",
    "        a       = np.array([i for sublist in Z for i in sublist ])\n",
    "        b       = np.zeros((a.size, a.max()+1))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return(b)\n",
    "\n",
    "    def perplexity(Theta, Beta, count_matrix):\n",
    "        '''\n",
    "        Calculate perplexity for given sample\n",
    "        '''\n",
    "        ltb     = np.log(Theta.dot(Beta))\n",
    "        num     = np.sum(count_matrix.multiply(ltb))\n",
    "        denom   = len(s)\n",
    "        return np.exp(-num/denom)\n",
    "\n",
    "    # Get params needed for passing to sampling functions\n",
    "    s       = [i for sublist in stemmed for i in sublist ]\n",
    "    vocab   = get_vocab(stemmed)\n",
    "    D       = len(stemmed)\n",
    "    V       = len(vocab)\n",
    "    idx     = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = make_count(stemmed, idx)\n",
    "    perp   = []\n",
    "\n",
    "    # Initialise params\n",
    "    if eta == None:\n",
    "        eta = 200/V\n",
    "    if alpha == None:\n",
    "        alpha = 50/K\n",
    "\n",
    "    Theta   = dirichlet(alpha = [alpha]*K, size = D)\n",
    "    Beta    = dirichlet(alpha = [eta]*V, size = K)\n",
    "    Z       = Z_class_1(Beta, Theta)\n",
    "    labels  = np.zeros((n_samples, len(s)))\n",
    "\n",
    "    # SAMPLING\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(burnin)):\n",
    "        Z       = Z_class_1(Beta, Theta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta, Beta, count_matrix))\n",
    "            #print('Burnin iteration {}'.format(i))\n",
    "\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(m*n_samples)):\n",
    "        Z       = Z_class_1(Beta, Theta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "\n",
    "        # Add every m-th sample to output\n",
    "        if i%m == 0:\n",
    "            Z_s = [i for sublist in Z for i in sublist ]\n",
    "            j = np.int(i/m)\n",
    "            labels[j, :] = Z_s\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta, Beta, count_matrix))\n",
    "            #print( \"Iteration {}\".format(i))\n",
    "\n",
    "    return (labels, perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 10:31:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [14:24<00:00,  1.38it/s]\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 10:46:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [22:28<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "LDA_labels, perp = Gibbs_sampling_LDA(stemmed,\n",
    "                                      K = 10,\n",
    "                                      n_samples = 500,\n",
    "                                      perplexity=True,\n",
    "                                      burnin = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see this sampling function takes around 35 minutes to complete 2500 iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "The objective of this exercise is to run the collapsed Gibbs sampling version for fitting an LDA model to the same data, hyperparameters and K, and compare its predictive distribution with the one we get from uncollapes Gibbs sampling. In order to do so, we will work with a Python3 adapted version of the collapsed Gibbs sampler that can be found in https: //github.com/sekhansen/text-mining-tutorial \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Plot the perplexity across sampling iterations. Which algorithm appears to burn in faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import topicmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/Laura/Desktop/text_mining_hw2/ex1')\n",
    "from utils import data_processing, get_vocab, make_count\n",
    "os.chdir('/Users/Laura/Desktop/text_mining_hw2')\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data_post1945 = data.loc[data.year >= 1945]\n",
    "%time stemmed, processed_data = data_processing(data_post1945)\n",
    "\n",
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "\n",
    "\n",
    "ldaobj.sample(0, 20, 75)\n",
    "\n",
    "perp2 = ldaobj.perplexity()\n",
    "perp_2 =  pd.DataFrame(perp_2)\n",
    "pd.DataFrame.to_csv(perp_2,path_or_buf='perplexity_collapsed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import perplexity computed with uncollapsed Gibbs sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/Laura/Desktop/text_mining_hw2/data')\n",
    "perp1 = pd.read_csv('./perplexity.csv') \n",
    "perp1 = np.array(perp1)\n",
    "perp1 = perp1[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the results by plotting the perplexity obtained with each algorithm against the samples taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(perp1,   lw = 1., label = 'uncollapsed')\n",
    "plt.plot(perp2, lw = 1., label = 'collapsed')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(0.65, 0.8))\n",
    "plt.ylabel('perplexity')\n",
    "plt.xlabel('samples')\n",
    "plt.savefig('perplexities.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the collapsed Gibbs sampler stabilizes rapidly, at 10 samples  and with a perplexity of 1130 more or less, the uncollapsed Gibbs sampler seems not to fully stabilize with even more than 75 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) After the burn-in period, construct estimates of the predictive distribution of theta for each document across a number of draws from the samplers. Are the average values of these predictive distributions similar in the uncollapsed and collapsed samplers? How variable are these predictive distributions in the two algorithms across sample draws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "ldaobj.sample(1000, 5, 100)\n",
    "\n",
    "\n",
    "k = ldaobj.K\n",
    "alpha =  ldaobj.alpha\n",
    "docterms = ldaobj.dt_avg()\n",
    "nm = np.array([len(doc) for doc in stemmed])\n",
    "nm = nm.reshape((10252,1))\n",
    "nmz = nm*docterms\n",
    "\n",
    "theta_collapsed = (nmz+alpha)/(nm+k*alpha)\n",
    "\n",
    "theta_uncoll = pd.read_csv('./theta_uncollapsed.csv') \n",
    "theta_uncoll.drop( 'Unnamed: 0',axis=1,inplace=True)\n",
    "theta_uncoll = np.array(theta_uncoll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proced to compare the average value of the predictive distributions in the uncollapsed and collapsed samplers. Also, we take a step more and compute the standard deviation and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_collapsed.mean()\n",
    "theta_uncoll.mean()\n",
    "\n",
    "# from collapsed gibbs sampler\n",
    "m_c = np.array(theta_collapsed.mean(axis=0))\n",
    "st_c= np.array(theta_collapsed.std(axis=0))\n",
    "var_c = np.array(np.var(theta_collapsed, axis=0))\n",
    "\n",
    "# from uncollapsed gibbs sampler\n",
    "m_u = np.array(theta_uncoll.mean(axis=0))\n",
    "st_u = np.array(theta_uncoll.std(axis=0))\n",
    "var_u = np.array(np.var(theta_uncoll, axis=0))\n",
    "\n",
    "basic_stats = pd.DataFrame(np.vstack((m_c,m_u,st_c,st_u,var_c,var_u)).T)\n",
    "basic_stats.columns = ['mean_c','mean_unc','sd_c','sd_unc','var_c','var_u']\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations from these results are:\n",
    "- The average value of the predictive distribution with the uncollapsed and collapsed gibbs samplers for each of the 10 topics are very similar, as well as the mean of the distribution which is 0.0999999 for both cases.\n",
    "- The standard deviation across topics and for each model is rather different, being bigger for the uncollapsed sampler. \n",
    "- The variance is significantly much smaller for each topic when working with the collapsed gibbs sampler than with the uncollapsed sampler indicating that the predictive distributions for for the collapsed case are more peaked and thin.\n",
    "\n",
    "Indeed, we can see such difference in the width of topic distributions for each sampler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collapsed gibbs sampler predictive distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collapsed gibbs sampler\n",
    "\n",
    "t1 = theta_collapsed[:,0]\n",
    "t2 = theta_collapsed[:,1]\n",
    "t3 = theta_collapsed[:,2]\n",
    "t4 = theta_collapsed[:,3]\n",
    "t5 = theta_collapsed[:,4]\n",
    "t6 = theta_collapsed[:,5]\n",
    "t7 = theta_collapsed[:,6]\n",
    "t8 = theta_collapsed[:,7]\n",
    "t9 = theta_collapsed[:,8]\n",
    "t10 = theta_collapsed[:,9]\n",
    "\n",
    "bins = np.linspace(0.03, 0.225, 70)\n",
    "\n",
    "pyplot.hist(t1, bins, alpha=0.5, label='topic 1')\n",
    "pyplot.hist(t2, bins, alpha=0.5, label='topic 2')\n",
    "pyplot.hist(t3, bins, alpha=0.5, label='topic 3')\n",
    "pyplot.hist(t4, bins, alpha=0.5, label='topic 4')\n",
    "pyplot.hist(t5, bins, alpha=0.5, label='topic 5')\n",
    "pyplot.hist(t6, bins, alpha=0.5, label='topic 6')\n",
    "pyplot.hist(t7, bins, alpha=0.5, label='topic 7')\n",
    "pyplot.hist(t8, bins, alpha=0.5, label='topic 8')\n",
    "pyplot.hist(t9, bins, alpha=0.5, label='topic 9')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncollapsed gibbs sampler predictive distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncollapsed gibbs sampler\n",
    "\n",
    "ut1 = theta_uncoll[:,0]\n",
    "ut2 = theta_uncoll[:,1]\n",
    "ut3 = theta_uncoll[:,2]\n",
    "ut4 = theta_uncoll[:,3]\n",
    "ut5 = theta_uncoll[:,4]\n",
    "ut6 = theta_uncoll[:,5]\n",
    "ut7 = theta_uncoll[:,6]\n",
    "ut8 = theta_uncoll[:,7]\n",
    "ut9 = theta_uncoll[:,8]\n",
    "ut10 = theta_uncoll[:,9]\n",
    "\n",
    "bins = np.linspace(0.03, 0.225, 70)\n",
    "\n",
    "pyplot.hist(ut1, bins, alpha=0.5, label='topic 1')\n",
    "pyplot.hist(ut2, bins, alpha=0.5, label='topic 2')\n",
    "pyplot.hist(ut3, bins, alpha=0.5, label='topic 3')\n",
    "pyplot.hist(ut4, bins, alpha=0.5, label='topic 4')\n",
    "pyplot.hist(ut5, bins, alpha=0.5, label='topic 5')\n",
    "pyplot.hist(ut6, bins, alpha=0.5, label='topic 6')\n",
    "pyplot.hist(ut7, bins, alpha=0.5, label='topic 7')\n",
    "pyplot.hist(ut8, bins, alpha=0.5, label='topic 8')\n",
    "pyplot.hist(ut9, bins, alpha=0.5, label='topic 9')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are interested in comparing the classification performance of a penalized logistic regression when paragraphs (which is associated with one of two political parties) are represented as unigram counts over raw terms versus topic shares. We use training samples to estimate the relationship between document content and political party, and then assess its out-of-sample performance on held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
