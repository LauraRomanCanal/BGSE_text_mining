{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BGSE Text Mining Homework 2\n",
    "### Euan Dowers, Veronika Kyuchukova, and Laura Roman\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "The objective of this exercise is to implement uncollapsed gibbs sampling for fitting an LDA model to state of the union speeches from 1945 onwards, with documents being defined at the paragraph level. \n",
    "\n",
    "First, we need to read in and process the data, as in the first homework set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse as ssp\n",
    "import time\n",
    "import matplotlib\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from numpy.random import dirichlet, multinomial\n",
    "from collections import Counter\n",
    "from utils import data_processing, get_vocab, make_count\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.57 s, sys: 36 ms, total: 8.6 s\n",
      "Wall time: 8.83 s\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table(\"HW1/speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data_post1945 = data.loc[data.year >= 1945]\n",
    "%time stemmed, processed_data = data_processing(data_post1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will create a function that implements uncollapsed gibbs sampling on our processed data.\n",
    "This essentially works by repeatedly sampling from the posterior distributions of $Z$, $\\Theta$, and $\\beta$ and updating values using the most recent sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Gibbs_sampling_LDA(stemmed, K, alpha = None, eta = None, m=3, n_samples = 200, burnin = 500, perplexity = False):\n",
    "    '''\n",
    "    Gibbs sampler for LDA model\n",
    "    '''\n",
    "\n",
    "    def Z_sample(Beta, Theta, Theta_x_Beta):\n",
    "        denom = Theta_x_Beta\n",
    "        Z = [0] * len(stemmed)\n",
    "        for i in range(len(stemmed)):\n",
    "            sel = [idx[word] for word in stemmed[i]]\n",
    "            prod = Beta[:,sel].T * Theta[i,:].reshape((1,K))\n",
    "            probs = prod / denom[i,sel].reshape(len(stemmed[i]),1)\n",
    "            Z[i] = [np.argmax(multinomial(n=1, pvals = z, size = 1)[0]) for z in probs]\n",
    "        return Z\n",
    "\n",
    "    def Beta_sample(eta, Z):\n",
    "        z_s = [z for sublist in Z for z in sublist ]\n",
    "        M = np.zeros(shape=(K,V))\n",
    "        for k in range(K):\n",
    "            words = [s[i] for i in range(len(z_s)) if z_s[i] == k]\n",
    "            counts = Counter(words)\n",
    "            for word in set(words):\n",
    "                M[k,idx[word]] = counts[word]\n",
    "        Beta = [dirichlet(alpha = eta + M[i],size = 1)[0] for i in range(K)]\n",
    "        return np.array(Beta)\n",
    "\n",
    "    def Theta_sample(alpha, Z):\n",
    "        N   = np.zeros(shape=(D,K))\n",
    "        for i in range(D):\n",
    "            counts   = Counter(Z[i])\n",
    "            for j in set(counts.keys()):\n",
    "                N[i,j]  = counts[j]\n",
    "        Theta = [dirichlet(alpha = alpha + N[i],size = 1)[0] for i in range(D)]\n",
    "        return np.array(Theta)\n",
    "\n",
    "    def perplexity(Theta_x_Beta, count_matrix):\n",
    "        '''\n",
    "        Calculate perplexity for given sample\n",
    "        '''\n",
    "        ltb     = np.log(Theta_x_Beta)\n",
    "        num     = np.sum(count_matrix.multiply(ltb))\n",
    "        denom   = len(s)\n",
    "        return np.exp(-num/denom)\n",
    "\n",
    "    # Get params needed for passing to sampling functions\n",
    "    s       = [i for sublist in stemmed for i in sublist ]\n",
    "    vocab   = get_vocab(stemmed)\n",
    "    D       = len(stemmed)\n",
    "    V       = len(vocab)\n",
    "    idx     = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = make_count(stemmed, idx)\n",
    "    perp   = []\n",
    "\n",
    "    # Initialise params\n",
    "    if eta == None:\n",
    "        eta = 200/V\n",
    "    if alpha == None:\n",
    "        alpha = 50/K\n",
    "\n",
    "    Theta   = dirichlet(alpha = [alpha]*K, size = D)\n",
    "    Beta    = dirichlet(alpha = [eta]*V, size = K)\n",
    "    Theta_x_Beta = Theta.dot(Beta)\n",
    "    Z       = Z_sample(Beta, Theta, Theta_x_Beta)\n",
    "    labels  = np.zeros((n_samples, len(s)))\n",
    "\n",
    "    # SAMPLING\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(burnin)):\n",
    "        Z       = Z_sample(Beta, Theta, Theta_x_Beta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "        Theta_x_Beta = Theta.dot(Beta)\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta_x_Beta, count_matrix))\n",
    "            #print('Burnin iteration {}'.format(i))\n",
    "\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(m*n_samples)):\n",
    "        Z       = Z_sample(Beta, Theta, Theta_x_Beta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "        Theta_x_Beta = Theta.dot(Beta)\n",
    "\n",
    "        # Add every m-th sample to output\n",
    "        if i%m == 0:\n",
    "            Z_s = [i for sublist in Z for i in sublist ]\n",
    "            j = np.int(i/m)\n",
    "            labels[j, :] = Z_s\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta_x_Beta, count_matrix))\n",
    "            #print( \"Iteration {}\".format(i))\n",
    "\n",
    "    return (labels, perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 06:39:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [51:10<00:00,  2.83s/it] \n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 07:30:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [25:20<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "LDA_labels, perp = Gibbs_sampling_LDA(stemmed,\n",
    "                                      K = 10,\n",
    "                                      n_samples = 100,\n",
    "                                      m = 5,\n",
    "                                      perplexity=True,\n",
    "                                      burnin = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see this sampling function takes around 1h10m to complete 1500 iterations. The output is a list of perplexities (from every 20th sample) and a numpy array containing a label assignment for each word in the corpus, for each sampling iteration after the initial burnin period. Since this takes so long we have saved these outputs and will load them from disk for any further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_labels.tofile('LDA_labels.npy')\n",
    "pd.DataFrame(perp).to_csv('perplexity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a function that, given a draw from the gibbs sampling, returns a document topic matrix (for use in the next exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_label = [[i]*len(stemmed[i]) for i in range(len(stemmed))]\n",
    "doc_label = [i for sublist in doc_label for i in sublist]\n",
    "\n",
    "def dt_matrix(labels, doc_label):\n",
    "    dt = np.zeros((len(set(doc_label)), K))\n",
    "    label_dict = dict(zip(range(labels.shape[0]), labels.astype(int)))\n",
    "    doc_dict = dict(zip(range(len(doc_label)), doc_label))\n",
    "    for i in range(len(doc_label)):\n",
    "        dt[doc_dict[i], label_dict[i]] += 1\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this to come up with some kind of estimate of the predictive distribution of theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 50/10\n",
    "K = 10\n",
    "dt_avg = np.zeros((len(stemmed), 10))\n",
    "for i in np.arange(0,100, 10 ):\n",
    "    dt_avg += dt_matrix(LDA_labels[i], doc_label)\n",
    "dt_avg /= 10\n",
    "\n",
    "nmz = dt_avg\n",
    "nm = np.array([len(doc) for doc in stemmed]).reshape(10252,1)\n",
    "\n",
    "theta_unc = (dt_avg + alpha) / (nm + 10 * alpha)\n",
    "\n",
    "theta_unc = pd.DataFrame(theta_uncollapsed)\n",
    "theta_unc.to_csv('theta_uncollapsed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate document term matrices for a number of draws and then average them to estimate the predictive distribution of $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "The objective of this exercise is to run the collapsed gibbs sampling version for fitting an LDA model to the same data, hyperparameters and K. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will work with a Python3 adapted version of the collapsed Gibbs sampler that can be found in https: //github.com/sekhansen/text-mining-tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "a) Plot the perplexity across sampling iterations. Which algorithm appears to burn in faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import topicmodels\n",
    "\n",
    "# 2.1 \n",
    "\n",
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "\n",
    "\n",
    "ldaobj.sample(0, 20, 75)\n",
    "\n",
    "\n",
    "perp_2 = ldaobj.perplexity()\n",
    "\n",
    "perp_1 = perp\n",
    "\n",
    "\n",
    "plt.plot(perp_1,   lw = 1., label = 'uncollapsed')\n",
    "plt.plot(perp_2, lw = 1., label = 'collapsed')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.ylabel('perplexity')\n",
    "plt.savefig('perplexities_collapsed.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# 2.2\n",
    "\n",
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "ldaobj.sample(1000, 5, 100)\n",
    "\n",
    "\n",
    "k = ldaobj.K\n",
    "alpha =  ldaobj.alpha\n",
    "\n",
    "docterms = ldaobj.dt_avg()\n",
    "nm = np.array([len(doc) for doc in stemmed])\n",
    "nm = nm.reshape((10252,1))\n",
    "nmz = nm*docterms\n",
    "nmz\n",
    "\n",
    "theta_coll = (nmz+alpha)/(nm+k*alpha)\n",
    "\n",
    "#compare theta for different topics and map collapsed-uncollapsed\n",
    "theta.sum(axis=0)\n",
    "\n",
    "#from uncollapsed \n",
    "theta_uncoll = theta_uncollapsed\n",
    "\n",
    "theta_uncoll.drop( 'Unnamed: 0',axis=1,inplace=True)\n",
    "theta_uncoll.sum(axis=0)\n",
    "\n",
    "theta_uncoll= np.array(theta_uncoll)\n",
    "\n",
    "# idea 2: compar mean and sd\n",
    "\n",
    "m1 = theta.mean(axis=0)\n",
    "st1= theta.std(axis=0)\n",
    "\n",
    "m2 = theta_uncoll.mean(axis=0)\n",
    "st2 = theta_uncoll.std(axis=0)\n",
    "data_compare = pd.DataFrame([m1,m2,st1,st2])\n",
    "\n",
    "# idea 3: compare the histograms/plots\n",
    "\n",
    "t1 = theta[:,0]\n",
    "t2 = theta[:,1]\n",
    "t3 = theta[:,2]\n",
    "t4 = theta[:,3]\n",
    "t5 = theta[:,4]\n",
    "t6 = theta[:,5]\n",
    "t7 = theta[:,6]\n",
    "t8 = theta[:,7]\n",
    "t9 = theta[:,8]\n",
    "t10 = theta[:,9]\n",
    "\n",
    "ut1 = theta_uncoll[:,0]\n",
    "ut2 = theta_uncoll[:,1]\n",
    "ut3 = theta_uncoll[:,2]\n",
    "ut4 = theta_uncoll[:,3]\n",
    "ut5 = theta_uncoll[:,4]\n",
    "ut6 = theta_uncoll[:,5]\n",
    "ut7 = theta_uncoll[:,6]\n",
    "ut8 = theta_uncoll[:,7]\n",
    "ut9 = theta_uncoll[:,8]\n",
    "ut10 = theta_uncoll[:,9]\n",
    "\n",
    "\n",
    "#T1-UT2\n",
    "#T2-UT5\n",
    "\n",
    "\n",
    "bins = np.linspace(0.03, 0.225, 70)\n",
    "\n",
    "pyplot.hist(t8, bins, alpha=0.5, label='collapsed')\n",
    "pyplot.hist(ut1, bins, alpha=0.5, label='uncoll, t1')\n",
    "pyplot.hist(ut2, bins, alpha=0.5, label='uncoll, t2')\n",
    "pyplot.hist(ut3, bins, alpha=0.5, label='uncoll, t3')\n",
    "pyplot.hist(ut4, bins, alpha=0.5, label='uncoll, t4')\n",
    "pyplot.hist(ut5, bins, alpha=0.5, label='uncoll, t5')\n",
    "pyplot.hist(ut6, bins, alpha=0.5, label='uncoll, t6')\n",
    "pyplot.hist(ut7, bins, alpha=0.5, label='uncoll, t7')\n",
    "pyplot.hist(ut8, bins, alpha=0.5, label='uncoll, t8')\n",
    "pyplot.hist(ut9, bins, alpha=0.5, label='uncoll, t9')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
