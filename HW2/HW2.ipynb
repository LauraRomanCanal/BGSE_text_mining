{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGSE Text Mining Homework 2\n",
    "### Euan Dowers, Veronika Kyuchukova, and Laura Roman\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "The objective of this exercise is to implement uncollapsed gibbs sampling for fitting an LDA model to state of the union speeches from 1945 onwards, with documents being defined at the paragraph level. \n",
    "\n",
    "First, we need to read in and process the data, as in the first homework set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse as ssp\n",
    "import time\n",
    "import matplotlib\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from numpy.random import dirichlet\n",
    "from collections import Counter\n",
    "from utils import data_processing, get_vocab, make_count\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.9 s, sys: 16 ms, total: 8.92 s\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table(\"HW1/speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data_post1945 = data.loc[data.year >= 1945]\n",
    "%time stemmed, processed_data = data_processing(data_post1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function that implements uncollapsed gibbs sampling on our processed data.\n",
    "This essentially works by repeatedly sampling from the posterior distributions of $Z$, $\\Theta$, and $\\beta$ and updating values using the most recent sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Gibbs_sampling_LDA(stemmed, K, alpha = None, eta = None, m=3, n_samples = 200, burnin = 500, perplexity = False):\n",
    "    '''\n",
    "    Gibbs sampler for LDA model\n",
    "    '''\n",
    "\n",
    "    def Z_class_1(Beta, Theta):\n",
    "        Z = [np.ndarray.tolist( np.argmax( Beta[:,[idx[word] for word in stemmed[i]]] * \\\n",
    "        Theta[i,:].reshape((K, 1)), axis = 0) ) for i in range(Theta.shape[0] )]\n",
    "        return Z\n",
    "\n",
    "    def Beta_sample(eta, Z):\n",
    "        z_s = [z for sublist in Z for z in sublist ]\n",
    "        M = np.zeros(shape=(K,V))\n",
    "        for k in range(K):\n",
    "            words = [s[i] for i in range(len(z_s)) if z_s[i] == k]\n",
    "            counts = Counter(words)\n",
    "            for word in set(words):\n",
    "                M[k,idx[word]] = counts[word]\n",
    "        Beta = [dirichlet(alpha = eta + M[i],size = 1)[0] for i in range(K)]\n",
    "        return np.array(Beta)\n",
    "\n",
    "    def Theta_sample(alpha, Z):\n",
    "        N   = np.zeros(shape=(D,K))\n",
    "        for i in range(D):\n",
    "            counts   = Counter(Z[i])\n",
    "            for j in set(counts.keys()):\n",
    "                N[i,j]  = counts[j]\n",
    "        Theta = [dirichlet(alpha = alpha + N[i],size = 1)[0] for i in range(D)]\n",
    "        return np.array(Theta)\n",
    "\n",
    "    def onehotencode(Z):\n",
    "        '''\n",
    "        Create function to one-hot encode topic allocation\n",
    "        '''\n",
    "        a       = np.array([i for sublist in Z for i in sublist ])\n",
    "        b       = np.zeros((a.size, a.max()+1))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return(b)\n",
    "\n",
    "    def perplexity(Theta, Beta, count_matrix):\n",
    "        '''\n",
    "        Calculate perplexity for given sample\n",
    "        '''\n",
    "        ltb     = np.log(Theta.dot(Beta))\n",
    "        num     = np.sum(count_matrix.multiply(ltb))\n",
    "        denom   = len(s)\n",
    "        return np.exp(-num/denom)\n",
    "\n",
    "    # Get params needed for passing to sampling functions\n",
    "    s       = [i for sublist in stemmed for i in sublist ]\n",
    "    vocab   = get_vocab(stemmed)\n",
    "    D       = len(stemmed)\n",
    "    V       = len(vocab)\n",
    "    idx     = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = make_count(stemmed, idx)\n",
    "    perp   = []\n",
    "\n",
    "    # Initialise params\n",
    "    if eta == None:\n",
    "        eta = 200/V\n",
    "    if alpha == None:\n",
    "        alpha = 50/K\n",
    "\n",
    "    Theta   = dirichlet(alpha = [alpha]*K, size = D)\n",
    "    Beta    = dirichlet(alpha = [eta]*V, size = K)\n",
    "    Z       = Z_class_1(Beta, Theta)\n",
    "    labels  = np.zeros((n_samples, len(s)))\n",
    "\n",
    "    # SAMPLING\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(burnin)):\n",
    "        Z       = Z_class_1(Beta, Theta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta, Beta, count_matrix))\n",
    "            #print('Burnin iteration {}'.format(i))\n",
    "\n",
    "    print('TIME:', time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    for i in tqdm(range(m*n_samples)):\n",
    "        Z       = Z_class_1(Beta, Theta)\n",
    "        Beta    = Beta_sample(eta, Z)\n",
    "        Theta   = Theta_sample(alpha, Z)\n",
    "\n",
    "        # Add every m-th sample to output\n",
    "        if i%m == 0:\n",
    "            Z_s = [i for sublist in Z for i in sublist ]\n",
    "            j = np.int(i/m)\n",
    "            labels[j, :] = Z_s\n",
    "        if i%20 == 0:\n",
    "            if perplexity:\n",
    "                perp.append(perplexity(Theta, Beta, count_matrix))\n",
    "            #print( \"Iteration {}\".format(i))\n",
    "\n",
    "    return (labels, perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 10:31:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [14:24<00:00,  1.38it/s]\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 10:46:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [22:28<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "LDA_labels, perp = Gibbs_sampling_LDA(stemmed,\n",
    "                                      K = 10,\n",
    "                                      n_samples = 500,\n",
    "                                      perplexity=True,\n",
    "                                      burnin = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see this sampling function takes around 35 minutes to complete 2500 iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "The objective of this exercise is to run the collapsed gibbs sampling version for fitting an LDA model to the same data, hyperparameters and K. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a Python3 adapted version of the collapsed Gibbs sampler that can be found in https: //github.com/sekhansen/text-mining-tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the perplexity across sampling iterations. Which algorithm appears to burn in faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import topicmodels\n",
    "\n",
    "# 2.1 \n",
    "\n",
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "\n",
    "\n",
    "ldaobj.sample(0, 20, 75)\n",
    "\n",
    "\n",
    "perp_2 = ldaobj.perplexity()\n",
    "\n",
    "perp_1 = perp\n",
    "\n",
    "\n",
    "plt.plot(perp_1,   lw = 1., label = 'uncollapsed')\n",
    "plt.plot(perp_2, lw = 1., label = 'collapsed')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.ylabel('perplexity')\n",
    "plt.savefig('perplexities_collapsed.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# 2.2\n",
    "\n",
    "ldaobj = topicmodels.LDA.LDAGibbs(stemmed, 10)\n",
    "ldaobj.sample(1000, 5, 100)\n",
    "\n",
    "\n",
    "k = ldaobj.K\n",
    "alpha =  ldaobj.alpha\n",
    "\n",
    "docterms = ldaobj.dt_avg()\n",
    "nm = np.array([len(doc) for doc in stemmed])\n",
    "nm = nm.reshape((10252,1))\n",
    "nmz = nm*docterms\n",
    "nmz\n",
    "\n",
    "theta_coll = (nmz+alpha)/(nm+k*alpha)\n",
    "\n",
    "#compare theta for different topics and map collapsed-uncollapsed\n",
    "theta.sum(axis=0)\n",
    "\n",
    "#from uncollapsed \n",
    "theta_uncoll = theta_uncollapsed\n",
    "\n",
    "theta_uncoll.drop( 'Unnamed: 0',axis=1,inplace=True)\n",
    "theta_uncoll.sum(axis=0)\n",
    "\n",
    "theta_uncoll= np.array(theta_uncoll)\n",
    "\n",
    "# idea 2: compar mean and sd\n",
    "\n",
    "m1 = theta.mean(axis=0)\n",
    "st1= theta.std(axis=0)\n",
    "\n",
    "m2 = theta_uncoll.mean(axis=0)\n",
    "st2 = theta_uncoll.std(axis=0)\n",
    "data_compare = pd.DataFrame([m1,m2,st1,st2])\n",
    "\n",
    "# idea 3: compare the histograms/plots\n",
    "\n",
    "t1 = theta[:,0]\n",
    "t2 = theta[:,1]\n",
    "t3 = theta[:,2]\n",
    "t4 = theta[:,3]\n",
    "t5 = theta[:,4]\n",
    "t6 = theta[:,5]\n",
    "t7 = theta[:,6]\n",
    "t8 = theta[:,7]\n",
    "t9 = theta[:,8]\n",
    "t10 = theta[:,9]\n",
    "\n",
    "ut1 = theta_uncoll[:,0]\n",
    "ut2 = theta_uncoll[:,1]\n",
    "ut3 = theta_uncoll[:,2]\n",
    "ut4 = theta_uncoll[:,3]\n",
    "ut5 = theta_uncoll[:,4]\n",
    "ut6 = theta_uncoll[:,5]\n",
    "ut7 = theta_uncoll[:,6]\n",
    "ut8 = theta_uncoll[:,7]\n",
    "ut9 = theta_uncoll[:,8]\n",
    "ut10 = theta_uncoll[:,9]\n",
    "\n",
    "\n",
    "#T1-UT2\n",
    "#T2-UT5\n",
    "\n",
    "\n",
    "bins = np.linspace(0.03, 0.225, 70)\n",
    "\n",
    "pyplot.hist(t8, bins, alpha=0.5, label='collapsed')\n",
    "pyplot.hist(ut1, bins, alpha=0.5, label='uncoll, t1')\n",
    "pyplot.hist(ut2, bins, alpha=0.5, label='uncoll, t2')\n",
    "pyplot.hist(ut3, bins, alpha=0.5, label='uncoll, t3')\n",
    "pyplot.hist(ut4, bins, alpha=0.5, label='uncoll, t4')\n",
    "pyplot.hist(ut5, bins, alpha=0.5, label='uncoll, t5')\n",
    "pyplot.hist(ut6, bins, alpha=0.5, label='uncoll, t6')\n",
    "pyplot.hist(ut7, bins, alpha=0.5, label='uncoll, t7')\n",
    "pyplot.hist(ut8, bins, alpha=0.5, label='uncoll, t8')\n",
    "pyplot.hist(ut9, bins, alpha=0.5, label='uncoll, t9')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
