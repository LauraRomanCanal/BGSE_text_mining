{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGSE Text Mining Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from numpy.linalg import svd\n",
    "from scipy.misc import logsumexp\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "QUESTION 1\n",
    "'''\n",
    "\n",
    "# Read in data\n",
    "# documents defined at the paragraph level\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "speeches = data['speech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokeniser(speeches):\n",
    "    # Tokenize speeches\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sp_tkn = [tokenizer.tokenize(speech) for speech in speeches]\n",
    "    return sp_tkn\n",
    "\n",
    "def remove_nonalph(sp_tkn):\n",
    "    # Remove non-alphabetic tokens\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j for j in sp_tkn[i] if j[0] in set(string.ascii_letters)]\n",
    "    return sp_tkn\n",
    "\n",
    "def stopword_del(sp_tkn):\n",
    "    # Remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j.lower() for j in sp_tkn[i] if j.lower() not in stop]\n",
    "    return sp_tkn\n",
    "\n",
    "def my_stem(sp_tkn):\n",
    "    # Stem words in documents\n",
    "    stemmer = porter.PorterStemmer()\n",
    "    stemmed = [[stemmer.stem(word) for word in doc] for doc in sp_tkn]\n",
    "    return stemmed\n",
    "\n",
    "def data_processing(speeches):\n",
    "    # Put together all other steps of data processing\n",
    "    sp_tkn = my_tokeniser(speeches)\n",
    "    sp_tkn = remove_nonalph(sp_tkn)\n",
    "    stemmed = my_stem(sp_tkn)\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed = data_processing(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fellow', 'citizen', 'of', 'the', 'senat', 'and', 'hous', 'of', 'repres']\n"
     ]
    }
   ],
   "source": [
    "print (stemmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
