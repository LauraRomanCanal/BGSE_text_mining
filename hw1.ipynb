{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BGSE Text Mining Homework 1\n",
    "Laura Roman, Veronika Kyuchukova and Euan Dowers\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from numpy.linalg import svd\n",
    "from scipy.misc import logsumexp\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "QUESTION 1\n",
    "'''\n",
    "\n",
    "# Read in data\n",
    "# documents defined at the paragraph level\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "speeches = data['speech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_tokeniser(speeches):\n",
    "    # Tokenize speeches\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sp_tkn = [tokenizer.tokenize(speech) for speech in speeches]\n",
    "    return sp_tkn\n",
    "\n",
    "def remove_nonalph(sp_tkn):\n",
    "    # Remove non-alphabetic tokens\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j for j in sp_tkn[i] if j[0] in set(string.ascii_letters)]\n",
    "    return sp_tkn\n",
    "\n",
    "def stopword_del(sp_tkn):\n",
    "    # Remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j.lower() for j in sp_tkn[i] if j.lower() not in stop]\n",
    "    return sp_tkn\n",
    "\n",
    "def my_stem(sp_tkn):\n",
    "    # Stem words in documents\n",
    "    stemmer = porter.PorterStemmer()\n",
    "    stemmed = [[stemmer.stem(word) for word in doc] for doc in sp_tkn]\n",
    "    return stemmed\n",
    "\n",
    "def remove_zerolen_strings(stemmed, data):\n",
    "    idx = [i for i in range(len(stemmed)) if len(stemmed[i]) == 0]\n",
    "    stemmed = [i for i in stemmed if len(i) > 0]\n",
    "    data = data.drop(data.index[idx])\n",
    "    #data = data.reset_index()\n",
    "    return [stemmed, data]\n",
    "\n",
    "def data_processing(data):\n",
    "    '''\n",
    "    Put together all steps in data processing. NOTE data must have column 'speech'\n",
    "    '''\n",
    "    speeches = data.speech\n",
    "    sp_tkn = my_tokeniser(speeches)\n",
    "    sp_tkn = remove_nonalph(sp_tkn)\n",
    "    sp_tkn = stopword_del(sp_tkn)\n",
    "    stemmed = my_stem(sp_tkn)\n",
    "    stemmed, data = remove_zerolen_strings(stemmed, data)\n",
    "    return [stemmed, data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CALCULATING TF-IDF SCORES\n",
    "\n",
    "def get_vocab(stemmed_data):\n",
    "    # extracts corpus vocabulary from list of documents\n",
    "    vocab = list(set().union(*stemmed_data))\n",
    "    return vocab\n",
    "\n",
    "def doc_count(stemmed,vocab):\n",
    "    # counts how many documents each word appears in\n",
    "    df = dict(zip(vocab,[0]*len(vocab)))\n",
    "    for i in range(len(stemmed)):\n",
    "        words = set(stemmed[i])\n",
    "        for j in words:\n",
    "            df[j] = df[j]+1\n",
    "    return df\n",
    "\n",
    "def make_IDF(stemmed,vocab):\n",
    "    # Calculates IDF factor for each word in vocabulary\n",
    "    D   = len(stemmed)\n",
    "    n   = len(get_vocab(stemmed))\n",
    "    df  = doc_count(stemmed,vocab)\n",
    "    IDF = [np.log(D/d) for d in df.values()]\n",
    "    IDF_dict = dict(zip(vocab,IDF))\n",
    "    return IDF_dict\n",
    "\n",
    "def make_count(stemmed):\n",
    "    vocab = get_vocab(stemmed)\n",
    "    D = len(stemmed)\n",
    "    n = len(vocab)\n",
    "    idx = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = np.ndarray(shape=(D,n))\n",
    "\n",
    "    for i in range(len(stemmed)):\n",
    "        for j in set(stemmed[i]):\n",
    "            count_matrix[i,idx[j]] = stemmed[i].count(j)\n",
    "    return count_matrix\n",
    "\n",
    "def corpus_tf(stemmed):\n",
    "    # Calculate corpus-level TF-IDF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    return tf\n",
    "\n",
    "def corpus_tf_idf(stemmed):\n",
    "    # Calculate corpus-level TF-IDF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    vocab = get_vocab(stemmed)\n",
    "    idf = list(make_IDF(stemmed, vocab).values())\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    tf_idf = tf * idf\n",
    "    return tf_idf\n",
    "\n",
    "def custom_stopword_del(stemmed, our_stopwords):\n",
    "    for i in range(len(stemmed)):\n",
    "        stemmed[i] = [j.lower() for j in stemmed[i] if j.lower() not in our_stopwords]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PROCESS THE DATA\n",
    "stemmed, processed_data = data_processing(data)\n",
    "\n",
    "#tf scores\n",
    "vocab = get_vocab(stemmed)\n",
    "tf_scores = corpus_tf(stemmed)\n",
    "\n",
    "sort_tf = sorted(tf_scores,reverse=True)\n",
    "ind_tf = sorted(range(len(tf_scores)), key=lambda k: tf_scores[k],reverse=True)\n",
    "vocab_s = [vocab[i] for i in ind_tf]\n",
    "\n",
    "term_sorttf = pd.DataFrame(\n",
    "    {'term': vocab_s,\n",
    "    'tf': sort_tf\n",
    "    })\n",
    "\n",
    "#tf-idf scores\n",
    "tf_idf_scores = corpus_tf_idf(stemmed)\n",
    "\n",
    "sort_tfidf = sorted(tf_idf_scores,reverse=True)\n",
    "ind_tfidf = sorted(range(len(tf_idf_scores)), key=lambda k: tf_idf_scores[k],reverse=True)\n",
    "vocab_sidf = [vocab[i] for i in ind_tfidf]\n",
    "#sorted tf_idf\n",
    "\n",
    "term_sortfidf = pd.DataFrame(\n",
    "    {'term': vocab_sidf,\n",
    "    'tf-idf': sort_tfidf\n",
    "    })\n",
    "\n",
    "our_stopwords = set(vocab_sidf[0:3000])\n",
    "\n",
    "stemmed = custom_stopword_del(stemmed, our_stopwords)\n",
    "stemmed, processed_data = remove_zerolen_strings(stemmed, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concord', 'auspici']\n",
      "I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity.\n"
     ]
    }
   ],
   "source": [
    "print(stemmed[0])\n",
    "print(processed_data.speech[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see from the example above the effect of our custom stopword removal, with words such as 'house' and 'senate' obviously context-specific. \n",
    "\n",
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def make_TF_IDF(stemmed):\n",
    "    # Calculates TF-IDF matrix\n",
    "    vocab = get_vocab(stemmed)\n",
    "    D = len(stemmed)\n",
    "    idx = dict(zip(vocab,range(len(vocab))))\n",
    "    IDF_dict = make_IDF(stemmed,vocab)\n",
    "    tf_idf = np.ndarray(shape=(D,len(vocab)))\n",
    "\n",
    "    for i in range(len(stemmed)):\n",
    "        for j in set(stemmed[i]):\n",
    "            tf_idf[i,idx[j]] = stemmed[i].count(j)*IDF_dict[j]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Comparison of parties post 1860\n",
    "\n",
    "# First collect names and assign parties to all presidents after first Republican president elected\n",
    "pres    = sorted(list ( set(data.loc[data.year > 1860].president)))\n",
    "party   = ['rep']*3 + ['dem']*3 + ['rep']*8 + ['dem']*3 + ['rep']*3 + ['dem']*1 + ['rep']*2 + ['dem'] + ['rep'] + ['dem']*2\n",
    "\n",
    "pres_party = dict(zip(pres, party))\n",
    "\n",
    "data_post1860 = processed_data.loc[processed_data.year > 1860]\n",
    "parties = [pres_party[i] for i in data_post1860.president]\n",
    "data_post1860 = data_post1860.assign(party=parties)\n",
    "\n",
    "stemmed_post1860, processed_post1860 = data_processing(data_post1860)\n",
    "stemmed_post1860 = custom_stopword_del(stemmed_post1860, our_stopwords)\n",
    "stemmed_post1860, processed_post1860 = remove_zerolen_strings(stemmed_post1860, processed_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parties_post1860 = [i for i in processed_post1860.party]\n",
    "dem_idx = [i for i in range(len(parties_post1860)) if parties_post1860[i] == 'dem']\n",
    "rep_idx = [i for i in range(len(parties_post1860)) if parties_post1860[i] == 'rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf_idf_post1860 = make_TF_IDF(stemmed_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(tf_idf_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "similarity_within_dem = cos_sim[dem_idx,:][:,dem_idx]\n",
    "similarity_within_rep = cos_sim[rep_idx,:][:,rep_idx]\n",
    "similarity_between_parties = cos_sim[dem_idx,:][:,rep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0150164028619\n",
      "0.0183860155548\n",
      "0.0156264565335\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(similarity_within_dem))\n",
    "print(np.mean(similarity_within_rep))\n",
    "print(np.mean(similarity_between_parties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Interestingly, this analysis gives that on average Democrats are more similar to Republicans than they are to other Democrats. This suggests that Democrats are more varied in their use of language than Republicans. \n",
    "\n",
    "This could perhaps be a result of the changing ideology of both parties over 150 years, from being the party of slavery to the party that elected the first black President. Perhaps it would make sense to analyse the difference between Republicans and Democrats from the mid 1960s onwards when the migration of Southern Democrats to the Republican party represented a change in ideology for both parties. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
