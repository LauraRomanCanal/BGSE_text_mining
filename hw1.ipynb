{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BGSE Text Mining Homework 1\n",
    "Laura Roman, Veronika Kyuchukova and Euan Dowers\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from numpy.linalg import svd\n",
    "from scipy.misc import logsumexp\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "QUESTION 1\n",
    "'''\n",
    "\n",
    "# Read in data\n",
    "# documents defined at the paragraph level\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "speeches = data['speech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_tokeniser(speeches):\n",
    "    # Tokenize speeches\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sp_tkn = [tokenizer.tokenize(speech) for speech in speeches]\n",
    "    return sp_tkn\n",
    "\n",
    "def remove_nonalph(sp_tkn):\n",
    "    # Remove non-alphabetic tokens\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j for j in sp_tkn[i] if j[0] in set(string.ascii_letters)]\n",
    "    return sp_tkn\n",
    "\n",
    "def stopword_del(sp_tkn):\n",
    "    # Remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j.lower() for j in sp_tkn[i] if j.lower() not in stop]\n",
    "    return sp_tkn\n",
    "\n",
    "def my_stem(sp_tkn):\n",
    "    # Stem words in documents\n",
    "    stemmer = porter.PorterStemmer()\n",
    "    stemmed = [[stemmer.stem(word) for word in doc] for doc in sp_tkn]\n",
    "    return stemmed\n",
    "\n",
    "def data_processing(speeches):\n",
    "    # Put together all other steps of data processing\n",
    "    sp_tkn = my_tokeniser(speeches)\n",
    "    sp_tkn = remove_nonalph(sp_tkn)\n",
    "    sp_tkn = stopword_del(sp_tkn)\n",
    "    stemmed = my_stem(sp_tkn)\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stemmed = data_processing(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and House of Representatives: \n",
      "['fellow', 'citizen', 'senat', 'hous', 'repres']\n"
     ]
    }
   ],
   "source": [
    "print(speeches[0])\n",
    "print(stemmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idx = [i for i in range(len(stemmed)) if len(stemmed[i])==0]\n",
    "stemmed = [stemmed[i] for i in range(len(stemmed)) if not i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CALCULATING TF-IDF SCORES\n",
    "\n",
    "def get_vocab(stemmed_data):\n",
    "    # extracts corpus vocabulary from list of documents\n",
    "    vocab = list(set().union(*stemmed_data))\n",
    "    return vocab\n",
    "\n",
    "def doc_count(stemmed,vocab):\n",
    "    # counts how many documents each word appears in\n",
    "    df = dict(zip(vocab,[0]*len(vocab)))\n",
    "    for i in range(len(stemmed)):\n",
    "        words = set(stemmed[i])\n",
    "        for j in words:\n",
    "            df[j] = df[j]+1\n",
    "    return df\n",
    "\n",
    "def make_IDF(stemmed,vocab):\n",
    "    # Calculates IDF factor for each word in vocabulary\n",
    "    D   = len(stemmed)\n",
    "    n   = len(get_vocab(stemmed))\n",
    "    df  = doc_count(stemmed,vocab)\n",
    "    IDF = [np.log(D/d) for d in df.values()]\n",
    "    IDF_dict = dict(zip(vocab,IDF))\n",
    "    return IDF_dict\n",
    "\n",
    "def make_count(stemmed):\n",
    "    vocab = get_vocab(stemmed)\n",
    "    D = len(stemmed)\n",
    "    n = len(vocab)\n",
    "    idx = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = np.ndarray(shape=(D,n))\n",
    "\n",
    "    for i in range(len(stemmed)):\n",
    "        for j in set(stemmed[i]):\n",
    "            count_matrix[i,idx[j]] = stemmed[i].count(j)\n",
    "    return count_matrix\n",
    "\n",
    "def corpus_tf(stemmed):\n",
    "    # Calculate corpus-level TF-IDF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    return tf\n",
    "\n",
    "def corpus_tf_idf(stemmed):\n",
    "    # Calculate corpus-level TF-IDF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    vocab = get_vocab(stemmed)\n",
    "    idf = list(make_IDF(stemmed, vocab).values())\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    tf_idf = tf * idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.67444203,  10.04962046,  25.05625212, ...,  29.94619004,\n",
       "        21.72203632,  22.02413233])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tf_idf(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
