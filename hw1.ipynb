{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BGSE Text Mining Homework 1\n",
    "### Laura Roman, Veronika Kyuchukova and Euan Dowers\n",
    "#### April 20, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1\n",
    "Before starting the text analysis we should pre-process the text, which can be done in the following steps: \n",
    "1. tokenize the data,\n",
    "2. remove non-alphabetic charavters, \n",
    "3. remove stopwords and\n",
    "4. stem the data\n",
    "\n",
    "Once the data is pre-processed we are in condition to compute the corpus level tf-idf score of every term (every paragrah) and choose a cutoof to remove words. And that's precisely what we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from numpy.linalg import svd\n",
    "from scipy.misc import logsumexp\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in data; documents defined at the paragraph level\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "speeches = data['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PRE-PROCESSING THE DATA\n",
    "def my_tokeniser(speeches):\n",
    "    # Tokenize speeches\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sp_tkn = [tokenizer.tokenize(speech) for speech in speeches]\n",
    "    return sp_tkn\n",
    "\n",
    "def remove_nonalph(sp_tkn):\n",
    "    # Remove non-alphabetic tokens\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j for j in sp_tkn[i] if j[0] in set(string.ascii_letters)]\n",
    "    return sp_tkn\n",
    "\n",
    "def stopword_del(sp_tkn):\n",
    "    # Remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for i in range(len(sp_tkn)):\n",
    "        sp_tkn[i] = [j.lower() for j in sp_tkn[i] if j.lower() not in stop]\n",
    "    return sp_tkn\n",
    "\n",
    "def my_stem(sp_tkn):\n",
    "    # Stem words in documents\n",
    "    stemmer = porter.PorterStemmer()\n",
    "    stemmed = [[stemmer.stem(word) for word in doc] for doc in sp_tkn]\n",
    "    return stemmed\n",
    "\n",
    "def remove_zerolen_strings(stemmed, data):\n",
    "    idx = [i for i in range(len(stemmed)) if len(stemmed[i]) == 0]\n",
    "    stemmed = [i for i in stemmed if len(i) > 0]\n",
    "    data = data.drop(data.index[idx])\n",
    "    data = data.reset_index(drop=True)\n",
    "    #return [stemmed, data]\n",
    "    return (stemmed, data)\n",
    "\n",
    "def data_processing(data):\n",
    "    #Put together all steps in data processing. NOTE data must have column 'speech'\n",
    "    speeches = data.speech\n",
    "    sp_tkn = my_tokeniser(speeches)\n",
    "    sp_tkn = remove_nonalph(sp_tkn)\n",
    "    sp_tkn = stopword_del(sp_tkn)\n",
    "    stemmed = my_stem(sp_tkn)\n",
    "    stemmed, data = remove_zerolen_strings(stemmed, data)\n",
    "    return (stemmed, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CALCULATING TF-IDF SCORES\n",
    "def get_vocab(stemmed_data):\n",
    "    # extracts corpus vocabulary from list of documents\n",
    "    vocab = list(set().union(*stemmed_data))\n",
    "    return vocab\n",
    "\n",
    "def doc_count(stemmed,vocab):\n",
    "    # counts how many documents each word appears in\n",
    "    df = dict(zip(vocab,[0]*len(vocab)))\n",
    "    for i in range(len(stemmed)):\n",
    "        words = set(stemmed[i])\n",
    "        for j in words:\n",
    "            df[j] = df[j]+1\n",
    "    return df\n",
    "\n",
    "def make_IDF(stemmed,vocab):\n",
    "    # Calculates IDF factor for each word in vocabulary\n",
    "    D   = len(stemmed)\n",
    "    n   = len(get_vocab(stemmed))\n",
    "    df  = doc_count(stemmed,vocab)\n",
    "    IDF = [np.log(D/d) for d in df.values()]\n",
    "    IDF_dict = dict(zip(vocab,IDF))\n",
    "    return IDF_dict\n",
    "\n",
    "def corpus_tf(stemmed):\n",
    "    # Calculate corpus-level TF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    return tf\n",
    "\n",
    "def corpus_tf_idf(stemmed):\n",
    "    # Calculate corpus-level TF-IDF scores\n",
    "    count_matrix = make_count(stemmed)\n",
    "    vocab = get_vocab(stemmed)\n",
    "    idf = list(make_IDF(stemmed, vocab).values())\n",
    "    tf = 1 +  np.log(np.sum(count_matrix, axis = 0))\n",
    "    tf_idf = tf * idf\n",
    "    return tf_idf\n",
    "\n",
    "def custom_stopword_del(stemmed, our_stopwords):\n",
    "    for i in range(len(stemmed)):\n",
    "        stemmed[i] = [j.lower() for j in stemmed[i] if j.lower() not in our_stopwords]\n",
    "    return stemmed\n",
    "\n",
    "def make_count(stemmed):\n",
    "    # Constructs docs-terms matrix\n",
    "    vocab = get_vocab(stemmed)\n",
    "    D = len(stemmed)\n",
    "    n = len(vocab)\n",
    "    idx = dict(zip(vocab,range(len(vocab))))\n",
    "    count_matrix = np.ndarray(shape=(D,n))\n",
    "\n",
    "    for i in range(len(stemmed)):\n",
    "        for j in set(stemmed[i]):\n",
    "            count_matrix[i,idx[j]] = stemmed[i].count(j)\n",
    "    return count_matrix\n",
    "\n",
    "def make_TF_IDF(stemmed):\n",
    "    # Calculates TF-IDF matrix\n",
    "    vocab = get_vocab(stemmed)\n",
    "    D = len(stemmed)\n",
    "    idx = dict(zip(vocab,range(len(vocab))))\n",
    "    IDF_dict = make_IDF(stemmed,vocab)\n",
    "    tf_idf = np.ndarray(shape=(D,len(vocab)))\n",
    "\n",
    "    for i in range(len(stemmed)):\n",
    "        for j in set(stemmed[i]):\n",
    "            tf_idf[i,idx[j]] = stemmed[i].count(j)*IDF_dict[j]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PROCESS THE DATA\n",
    "stemmed, processed_data = data_processing(data)\n",
    "\n",
    "#tf scores\n",
    "vocab = get_vocab(stemmed)\n",
    "tf_scores = corpus_tf(stemmed)\n",
    "\n",
    "sort_tf = sorted(tf_scores,reverse=True)\n",
    "ind_tf = sorted(range(len(tf_scores)), key=lambda k: tf_scores[k],reverse=True)\n",
    "vocab_s = [vocab[i] for i in ind_tf]\n",
    "\n",
    "term_sorttf = pd.DataFrame(\n",
    "    {'term': vocab_s,\n",
    "    'tf': sort_tf\n",
    "    })\n",
    "\n",
    "#tf-idf scores\n",
    "tf_idf_scores = corpus_tf_idf(stemmed)\n",
    "\n",
    "sort_tfidf = sorted(tf_idf_scores,reverse=True)\n",
    "ind_tfidf = sorted(range(len(tf_idf_scores)), key=lambda k: tf_idf_scores[k],reverse=True)\n",
    "vocab_sidf = [vocab[i] for i in ind_tfidf]\n",
    "#sorted tf_idf\n",
    "\n",
    "term_sortfidf = pd.DataFrame(\n",
    "    {'term': vocab_sidf,\n",
    "    'tf-idf': sort_tfidf\n",
    "    })\n",
    "\n",
    "our_stopwords = set(vocab_sidf[0:2000])\n",
    "\n",
    "stemmed = custom_stopword_del(stemmed, our_stopwords)\n",
    "stemmed, processed_data = remove_zerolen_strings(stemmed, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(stemmed[0])\n",
    "print(processed_data.speech[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see from the example above the effect of our custom stopword removal, with words such as 'house' and 'senate' obviously context-specific. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have processed the data and formed the document-term matrix, we are ready to perform some analytics that will help us gain insight on the text we have in our hands. In this first part, we will use dictionary methods that hopefully will explain the topics composition of US presidential speeches over years. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a )\n",
    "We have come up with 7 dictionaries (topics) of interest which are lists of positive, negative, uncertainty, passive, ethics, politics, economy and military words. We load these lists of words, stem them and clean them by getting rid of ambiguous words. That is, words that appear in more than one dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%run dict_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we can access these diccionaries with the variables\n",
    "positive_dict\n",
    "negative_dict\n",
    "uncert_dict\n",
    "passive_dict\n",
    "politic_dict\n",
    "econ_dict\n",
    "military\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. b)\n",
    "In order to provide a quantitative representation of each document, we can construct a documents-topics matrix where each row is a document and each of the seven columns represents a dictionary or topic. Followingly, the (i,j) matrix entry represents the number of words are classified as topic j in the document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def count_on_dict(dictionary, words):\n",
    "    # Calculate counts of a word list based on a dictionary\n",
    "    recognized_word_count = 0\n",
    "\n",
    "    words_list = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            recognized_word_count += 1\n",
    "            words_list.append(word)\n",
    "\n",
    "    return recognized_word_count, words_list\n",
    "\n",
    "def docs_dict_matrix(stem,positive_dict,negative_dict,ethic_dict,politic_dict,econ_dict,military_dict,uncert_dict,passive_dict ):\n",
    "    # Computes docs-topics matrix with data arranged by year or by docs\n",
    "    counts = np.ndarray(shape=(len(stem),8))\n",
    "    for j in range(len(stem)):\n",
    "        words = []\n",
    "        words = stem[j]\n",
    "        counts[j,0] = count_on_dict(positive_dict,words)[0]\n",
    "        counts[j,1] = count_on_dict(negative_dict,words)[0]\n",
    "        counts[j,2] = count_on_dict(uncert_dict,words)[0]\n",
    "        counts[j,3] = count_on_dict(passive_dict,words)[0]\n",
    "        counts[j,4] = count_on_dict(ethic_dict,words)[0]\n",
    "        counts[j,5] = count_on_dict(politic_dict,words)[0]\n",
    "        counts[j,6] = count_on_dict(econ_dict,words)[0]\n",
    "        counts[j,7] = count_on_dict(military_dict,words)[0]\n",
    "        #pos_words = calculate_sentiment_for_word_list(positive_dict,words)[1] # classif words\n",
    "    \n",
    "    counts = pd.DataFrame(counts, columns=['pos', 'neg', 'unc', 'passive', 'ethic', 'polit', 'econ', 'milit'])\n",
    "    counts['total'] = counts.sum(axis=1)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we obtaint the documents-topics matrix (dtm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dtm = docs_dict_matrix(stemmed,positive_dict,negative_dict,ethic_dict,politic_dict,econ_dict,military_dict,uncert_dict,passive_dict )\n",
    "dtm.shape\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "More illustrative is to show the percentage of the different topics across all documents (whole corpus).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sum_odocs = dtm.sum(axis=0)\n",
    "perc = np.ndarray(shape=(8,))\n",
    "for i in range(8):\n",
    "    perc[i]=100*sum_odocs[i]/sum_odocs[8]\n",
    "perc = pd.DataFrame(perc)\n",
    "perc.columns = ['%']\n",
    "perc.index =    ['positive', 'negative', 'uncertainty', 'passive', 'ethic', 'politics', 'economy', 'military']\n",
    "perc.sort_values(by='%', ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On the other hand, we are interested in studying the yearly evolution of the speeches content instead of that for each \n",
    "one of the paragraphs (or documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#yearly data processing\n",
    "data_by_years= pd.DataFrame(processed_data)\n",
    "data_by_years = data_by_years.groupby('year', sort=False, as_index=True)['speech'].apply(' '.join)\n",
    "data_by_years = data_by_years.reset_index()\n",
    "stemmed_y, processed_data_y = data_processing(data_by_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we obtain the year-topics matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yt = docs_dict_matrix(stemmed_y,positive_dict,negative_dict,ethic_dict,politic_dict,econ_dict,military_dict,uncert_dict,passive_dict )\n",
    "yt['year'] =data_by_years['year']\n",
    "yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That can aswell be expressed by percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ytp=yt\n",
    "ytp['pos']=100*ytp['pos']/ytp['total']; ytp['neg']=100*ytp['neg']/ytp['total']\n",
    "ytp['unc']=100*ytp['unc']/ytp['total']; ytp['passive']=100*ytp['passive']/ytp['total']\n",
    "ytp['ethic']=100*ytp['ethic']/ytp['total']; ytp['polit']=100*ytp['polit']/ytp['total']\n",
    "ytp['econ']=100*ytp['econ']/ytp['total']; ytp['milit']=100*ytp['milit']/ytp['total']\n",
    "ytp['total']=100; ytp['year'] =data_by_years['year']\n",
    "ytp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now wish to understand this data by visualizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_dates = [1812,1861,1865,1914,1918,1929,1941,1945, 1974,1991,2001,2008]\n",
    "us_dates_exp = ['War on Britain','Civil War','','WWI','', 'Black Thursday','WWII','','Watergate scandal','Iraq attacks','WTC attack', 'Great Recession']\n",
    "\n",
    "X = ytp['year']\n",
    "Y1 = ytp['pos'];Y2= ytp['neg'];Y3 = ytp['unc'];Y4= ytp['passive']\n",
    "Y5 = ytp['ethic'];Y6= ytp['polit'];Y7 = ytp['econ']; Y8= ytp['milit']\n",
    "plt.plot(X, Y1,   lw = 1., label = 'positive')\n",
    "plt.plot(X, Y2, lw = 1., label = 'negative')\n",
    "plt.plot(X, Y3, lw = 1., label = 'uncertainty')\n",
    "plt.plot(X, Y4, lw = 1., label = 'passive')\n",
    "plt.plot(X, Y5, lw = 1., label = 'ethic')\n",
    "plt.plot(X, Y6,  lw = 1., label = 'politics')\n",
    "plt.plot(X, Y7,  lw = 1., label = 'economics')\n",
    "plt.plot(X, Y8,  lw = 1., label = 'military')\n",
    "for i in range(len(us_dates)):\n",
    "    plt.axvline(us_dates[i],linestyle=\"dashed\", color=\"black\", lw=0.6)\n",
    "    plt.text(us_dates[i],-8,us_dates_exp[i],rotation=90)\n",
    "plt.ylabel('%')\n",
    "plt.title('Speeches topics evolution', y=1.08)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice we have marked important historical US dates that hopefully will somewhat be related with the content of the yearly president speeches. In order to see more clearly what is going on with the topic time series, we will disagreggate the previous plot for 4 different time periods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_dates = [1812, 1815]\n",
    "us_dates_exp = ['War on Britain', 'War on Britain ends']\n",
    "plt.plot(X[0:33], Y1[0:33],   lw = 1.5, label = 'positive')\n",
    "plt.plot(X[0:33], Y2[0:33], lw = 1.5, label = 'negative')\n",
    "#plt.plot(X[0:33], Y3[0:33], lw = 1.5, label = 'uncertainty')\n",
    "#plt.plot(X[0:33], Y4[0:33], lw = 1.5, label = 'passive')\n",
    "#plt.plot(X[0:33], Y5[0:33], lw = 1.5, label = 'ethics')\n",
    "plt.plot(X[0:33], Y6[0:33],  lw = 1.5, label = 'politics')\n",
    "plt.plot(X[0:33], Y7[0:33],  lw = 1.5, label = 'economics')\n",
    "plt.plot(X[0:33], Y8[0:33],  lw = 1.5, label = 'military')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "for i in range(len(us_dates)):\n",
    "    plt.axvline(us_dates[i], linestyle=\"dashed\", color=\"black\", lw=0.6)\n",
    "    plt.text(us_dates[i],-5,us_dates_exp[i],rotation=90)\n",
    "plt.ylabel('%')\n",
    "plt.title('Speeches topics evolution', y=1.08)\n",
    "plt.savefig('./figures/1820s.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The US-Britain war (aka The War of 1812) was a military conflict that lasted from June 1812 to February 1815, fought between the United States of America and the United Kingdom, its North American colonies, and its Native American allies. \n",
    "We can see that during this period yearly speeches were more military oriented and less economics oriented, which really makes if you're the president of a country on war. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_dates = [1861,1865]\n",
    "us_dates_exp = ['Civil War','Civil War ends']\n",
    "plt.plot(X[60:93], Y1[60:93],   lw = 1.5, label = 'positive')\n",
    "plt.plot(X[60:93], Y2[60:93], lw = 1.5, label = 'negative')\n",
    "#plt.plot(X[60:93], Y3[60:93], lw = 1.5, label = 'uncertainty')\n",
    "#plt.plot(X[60:93], Y4[60:93], lw = 1.5, label = 'passive')\n",
    "#plt.plot(X[60:93], Y5[60:93], lw = 1.5, label = 'ethics')\n",
    "plt.plot(X[60:93], Y6[60:93],  lw = 1.5, label = 'politics')\n",
    "plt.plot(X[60:93], Y7[60:93],  lw = 1.5, label = 'economics')\n",
    "plt.plot(X[60:93], Y8[60:93],  lw = 1.5, label = 'military')\n",
    "for i in range(len(us_dates)):\n",
    "    plt.axvline(us_dates[i], linestyle=\"dashed\", color=\"black\", lw=0.6)\n",
    "    plt.text(us_dates[i],-5,us_dates_exp[i],rotation=90)\n",
    "plt.ylabel('%')\n",
    "plt.title('Speeches topics evolution', y=1.08)\n",
    "plt.savefig('./figures/1850s.png', bbox_inches='tight')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The American Civil War was an internal conflict fought in the United States (U.S.) from 1861 to 1865. The Union (i.e., The United States) faced secessionists in eleven Southern states grouped together as the Confederate States of America. The Union won the war, which remains the bloodiest in U.S. history.\n",
    "\n",
    "Again, speeches are more militar oriented and economic terms are very dominant in the speeches, experiencing an increase of frequency during this period. The American Civil War had a very strong economic background as Southern whites believed that the emancipation of slaves would destroy the South's economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population, p.e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_dates = [ 1914,1918,1929,1941,1945]\n",
    "us_dates_exp = ['WWI','WWI ends', 'Black Thursday','WWII','WWII ends']\n",
    "plt.plot(X[120:170], Y1[120:170],   lw = 1.5, label = 'positive')\n",
    "plt.plot(X[120:170], Y2[120:170], lw = 1.5, label = 'negative')\n",
    "#plt.plot(X[120:170], Y3[120:170], lw = 1.5, label = 'uncertainty')\n",
    "#plt.plot(X[120:170], Y4[120:170], lw = 1.5, label = 'passive')\n",
    "#plt.plot(X[120:170], Y5[120:170], lw = 1.5, label = 'ethics')\n",
    "plt.plot(X[120:170], Y6[120:170],  lw = 1.5, label = 'politics')\n",
    "plt.plot(X[120:170], Y7[120:170],  lw = 1.5, label = 'economics')\n",
    "plt.plot(X[120:170], Y8[120:170], lw = 1.5, label = 'military')\n",
    "for i in range(len(us_dates)):\n",
    "    plt.axvline(us_dates[i], linestyle=\"dashed\", color=\"black\", lw=0.6)\n",
    "    plt.text(us_dates[i],-10,us_dates_exp[i],rotation=90)\n",
    "plt.ylabel('%')\n",
    "plt.title('Speeches topics evolution', y=1.08)\n",
    "plt.savefig('./figures/interest1900s.png', bbox_inches='tight')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "During the first half of the 90's speechecs became more and more economy-related. It is interesting to compare the composition of speeches in the First World War (1914-1918) with that in the Second World War (1941-1945). \n",
    "The United States declared war on Germany on April 6, 1917, during World War I. The U.S. was an independent power and did not officially join the Allies. It closely cooperated with them militarily but acted alone in diplomacy. This explains why, while a war was taking place, presidential speeches didn't become more military nor economically oriented but remained mainly as positive speeches. \n",
    "\n",
    "On the other hand, in the Second World War patriotism became the central theme of advertising throughout the war, as large scale campaigns were launched to sell war bonds, promote efficiency in factories, reduce ugly rumors, and maintain civilian morale. The media cooperated and the federal government presented the official view of the war. This can be clearly seen as speeches were mainly about military and during war were increasingly more positive and less negative to give courage to the population.\n",
    "\n",
    "The Black Tuesday (or The Wall Street Crash), began on October 24 1929 and was the most devastating stock market crash in the history of the United States, and signaled the beginning of the 12-year Great Depression that affected all Western industrialized countries. Compromising the following years presidential speeches to be even more economics-related than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_dates = [ 1974,1991,2001,2008]\n",
    "us_dates_exp = ['Watergate scandal','Iraq attacks','WTC attack', 'Great Recession']\n",
    "plt.plot(X[180:220], Y1[180:220],   lw = 1.5, label = 'positive')\n",
    "plt.plot(X[180:220], Y2[180:220], lw = 1.5, label = 'negative')\n",
    "#plt.plot(X[180:220], Y3[180:220], lw = 1.5, label = 'uncertainty')\n",
    "#plt.plot(X[180:220], Y4[180:220], lw = 1.5, label = 'passive')\n",
    "#plt.plot(X[180:220], Y5[180:220], lw = 1.5, label = 'ethics')\n",
    "plt.plot(X[180:220], Y6[180:220],  lw = 1.5, label = 'politics')\n",
    "plt.plot(X[180:220], Y7[180:220],  lw = 1.5, label = 'economics')\n",
    "plt.plot(X[180:220], Y8[180:220], lw = 1.5, label = 'military')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "for i in range(len(us_dates)):\n",
    "    plt.axvline(us_dates[i], linestyle=\"dashed\", color=\"black\", lw=0.6)\n",
    "    plt.text(us_dates[i],-8,us_dates_exp[i],rotation=90)\n",
    "plt.title('Speeches topics evolution', y=1.08)\n",
    "plt.savefig('./figures/2000s.png', bbox_inches='tight')\n",
    "plt.ylabel('%')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The immediate years after the unfortunate events that took place during 1970-2010, Watergate scandal, Iraq attacks, World Trade Center attach, and the Great Recession; speeches became less positive and more negative. \n",
    "\n",
    "After Iraq attacks, speeches didn't become more military oriented but less, as it was an offensive strategy of war on another land. However, after the World Trade Center attack where US was the victim speeches became very military-oriented as this attach would lead into more war-strategies.\n",
    "\n",
    "Again, after the Great Economic Recession speeches became more economic-oriented, less positive and more negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. c)\n",
    "We are interested in finding some correlations between relevant US time series and the addresses. We will study two different time series: unemployment in the US and US average interest rate in the period (1948 - 2010). We expect to find that unemployment is positively correlated with uncertainty in this period, whereas average interest rate should be negatively correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "uncert= ytp[ytp.year >= 1948].unc.reset_index(drop=True).values; posit= ytp[ytp.year >= 1948].pos.reset_index(drop=True).values\n",
    "negat= ytp[ytp.year >= 1948].neg.reset_index(drop=True).values; passive= ytp[ytp.year >= 1948].passive.reset_index(drop=True).values\n",
    "econ= ytp[ytp.year >= 1948].econ.reset_index(drop=True).values; polit= ytp[ytp.year >= 1948].polit.reset_index(drop=True).values\n",
    "milit= ytp[ytp.year >= 1948].milit.reset_index(drop=True).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' unemployment'''\n",
    "file = pd.read_table(\"./timeseries/annual_unemployment.txt\",header=None)\n",
    "unempl = pd.DataFrame(file[1]); unempl = unempl[1].values\n",
    "\n",
    "corr_unempl = [pearsonr(unempl, uncert), pearsonr(unempl, posit),pearsonr(unempl, negat),\n",
    "                pearsonr(unempl, passive) ,pearsonr(unempl, econ),pearsonr(unempl, polit),pearsonr(unempl, milit) ] \n",
    "corr_unempl = pd.DataFrame(corr_unempl)\n",
    "corr_unempl[2] = ['uncertainty', 'positive', 'negative', 'passive', 'economy', 'politics', 'military']; corr_unempl.columns = ('unempl corr', 'p-val','topic')\n",
    "corr_unempl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Correlation between unemployment and uncertainty in the US during 1948-2010 is positive as we expected, however from the p-value we see that such correlation is not significant. Also we have computed the correlation with the other topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''inflation rate'''\n",
    "file2 = pd.read_table(\"./timeseries/inflation_rate.txt\",header=None)\n",
    "infl = file2[file2[0]>=1948].reset_index(drop=True); infl = pd.DataFrame(infl[1]); infl = infl[1].values\n",
    "\n",
    "corr_infl = [pearsonr(infl, uncert), pearsonr(infl, posit),pearsonr(infl, negat),\n",
    "                pearsonr(infl, passive) ,pearsonr(infl, econ),pearsonr(infl, polit),pearsonr(infl, milit) ] \n",
    "corr_infl = pd.DataFrame(corr_infl)\n",
    "corr_infl[2] = ['uncertainty', 'positive', 'negative', 'passive', 'economy', 'politics', 'military']; corr_infl.columns = ('infl corr', 'p-val','topic')\n",
    "corr_infl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Correlation between inflation rate and uncertainty is negative as expected, and such correlation is significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. d)\n",
    "Using the same dictionaries we now want to compute the content of each yearly-document using term weighting instead of document-topic counts as we have already done. Because in Exercise 2.b) we didn't show top topic document rankings, we will do now for both dt scoring and tf-idf scoring, comparing the results and checking that top year-docs obained with this method coincides with what we obtained in the code previously shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def ranking(stemmed,data,dictionary, use_tf_idf, n):\n",
    "    # From DT matrix/TF-IDF matrix and a vocabulary, gets rid of words not appearing in the dictionary,\n",
    "    # computes the total frequency/weighted count for each doc (or year) and orders the docs by scoring\n",
    "    vocab = get_vocab(stemmed)\n",
    "    dt_matrix = make_count(stemmed)\n",
    "    tfidf_matrix = make_TF_IDF(stemmed)\n",
    "\n",
    "    if (use_tf_idf):\n",
    "        dtm = tfidf_matrix\n",
    "    else:\n",
    "        dtm = dt_matrix\n",
    "\n",
    "    dict_tokens_set = set(item for item in dictionary)\n",
    "    intersection = list(set(dict_tokens_set) & set(vocab))\n",
    "    vec_positions = [int(token in intersection) for token in vocab] \n",
    "\n",
    "    sums = np.zeros(len(dtm))\n",
    "    for j in range(len(dtm)):\n",
    "        sums[j] = sum([a * b for a, b in zip(dtm[j], vec_positions)])\n",
    "        #this sums vector is equal to the column of the given dictionary of the docs-dict matrix we obtain with docs_dict_matrix()\n",
    "        order = sorted(range(len(sums)), key = lambda k: sums[k], reverse=True)\n",
    "    ordered_year_data_n = [None] * len(dtm)\n",
    "    ordered_sums = np.zeros(len(dtm))\n",
    "\n",
    "    counter = 0        \n",
    "    for num in order:\n",
    "        ordered_year_data_n[counter] = data.year[num]\n",
    "        ordered_sums[counter] = sums[num]\n",
    "        counter += 1\n",
    "\n",
    "    return list((ordered_year_data_n[0:n], ordered_sums[0:n]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We proceed to rank top 5 year for each topic/dictionary and for both scoring methods, dt and tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_rank = 5\n",
    "\n",
    "# dt score\n",
    "pos_sorted_years,pos_tf_score = ranking(stemmed_y, processed_data_y, positive_dict, False, n_rank) \n",
    "neg_sorted_years,neg_tf_score = ranking(stemmed_y, processed_data_y, negative_dict, False, n_rank) \n",
    "et_sorted_years,et_tf_score = ranking(stemmed_y, processed_data_y, ethic_dict, False, n_rank) \n",
    "pol_sorted_years,pol_tf_score = ranking(stemmed_y, processed_data_y, politic_dict, False, n_rank) \n",
    "ec_sorted_years,ec_tf_score = ranking(stemmed_y, processed_data_y, econ_dict, False, n_rank) \n",
    "mil_sorted_years,mil_tf_score = ranking(stemmed_y, processed_data_y, military_dict, False, n_rank) \n",
    "unc_sorted_years,unc_tf_score = ranking(stemmed_y, processed_data_y, uncert_dict, False, n_rank) \n",
    "pas_sorted_years,pas_tf_score = ranking(stemmed_y, processed_data_y, passive_dict, False, n_rank) \n",
    "\n",
    "# tf-idf score\n",
    "ipos_sorted_years,ipos_tf_score = ranking(stemmed_y, processed_data_y, positive_dict,True,n_rank) \n",
    "ineg_sorted_years,ineg_tf_score = ranking(stemmed_y, processed_data_y, negative_dict,True,n_rank) \n",
    "iet_sorted_years,iet_tf_score = ranking(stemmed_y, processed_data_y, ethic_dict,True,n_rank) \n",
    "ipol_sorted_years,ipol_tf_score = ranking(stemmed_y, processed_data_y, politic_dict,True,n_rank) \n",
    "iec_sorted_years,iec_tf_score = ranking(stemmed_y, processed_data_y, econ_dict,True,n_rank) \n",
    "imil_sorted_years,imil_tf_score = ranking(stemmed_y, processed_data_y, military_dict,True,n_rank) \n",
    "iunc_sorted_years,iunc_tf_score = ranking(stemmed_y, processed_data_y, uncert_dict,True,n_rank) \n",
    "ipas_sorted_years,ipas_tf_score = ranking(stemmed_y, processed_data_y, passive_dict,True,n_rank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 economics years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(ec_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(ec_sorted_years[i], ec_tf_score[i],iec_sorted_years[i], iec_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods agree in the top 5 economic years raking, although obviously giving different scores, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 positive years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(pos_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(pos_sorted_years[i], pos_tf_score[i],ipos_sorted_years[i], ipos_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT and TF-IDF scoring give the same top 2 positivie years, being 1981 and 1980, but differ for the rest of the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 negative years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(neg_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(neg_sorted_years[i], neg_tf_score[i],ineg_sorted_years[i], round(ineg_tf_score[i],1))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funny enough, top 2 negative years are 1981, both scoring methods agreeing. With this dictionary, they even agree in the top 5 years but changing the order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 politics years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(pol_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(pol_sorted_years[i], pol_tf_score[i],ipol_sorted_years[i], ipol_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2 politics years are 1981, both scoring methods agreeing. With this dictionary, they even agree in the top 5 years but changing the order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 military years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(mil_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(mil_sorted_years[i], mil_tf_score[i],imil_sorted_years[i], imil_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding military year ranking, where the counting frequency is much smaller compared to using other dictionaries, the two different methods disagree in every year of the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 ethics years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(et_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(et_sorted_years[i], et_tf_score[i],iet_sorted_years[i], iet_tf_score[i])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both scoring methods only agree in the top 2 ethics years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 uncertainty years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(unc_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(unc_sorted_years[i], unc_tf_score[i],iunc_sorted_years[i], iunc_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total disaggreance between dt and tf-idf scring for top 10 uncertainty years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Top 5 passive years\")\n",
    "print (\"   \" , \"DT\", \"     \"    , \" TFIDF\")\n",
    "for i in range(len(pas_sorted_years)):\n",
    "    print (\"{0} {1} {2} {3}  \".format(pas_sorted_years[i], pas_tf_score[i],ipas_sorted_years[i], ipas_tf_score[i])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreeance between both methods for the top 2 passive years, 1981 and 1980."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Comparison of parties post 1860\n",
    "\n",
    "# First collect names and assign parties to all presidents after first Republican president elected\n",
    "pres    = sorted(list ( set(data.loc[data.year > 1860].president)))\n",
    "party   = ['rep']*3 + ['dem']*3 + ['rep']*8 + ['dem']*3 + ['rep']*3 + ['dem']*1 + ['rep']*2 + ['dem'] + ['rep'] + ['dem']*2\n",
    "\n",
    "pres_party = dict(zip(pres, party))\n",
    "\n",
    "data_post1860 = data.loc[data.year > 1860]\n",
    "parties = [pres_party[i] for i in data_post1860.president]\n",
    "data_post1860 = data_post1860.assign(party=parties)\n",
    "\n",
    "stemmed_post1860, processed_post1860 = data_processing(data_post1860)\n",
    "stemmed_post1860 = custom_stopword_del(stemmed_post1860, our_stopwords)\n",
    "stemmed_post1860, processed_post1860 = remove_zerolen_strings(stemmed_post1860, processed_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parties_post1860 = [i for i in processed_post1860.party]\n",
    "dem_idx = [i for i in range(len(parties_post1860)) if parties_post1860[i] == 'dem']\n",
    "rep_idx = [i for i in range(len(parties_post1860)) if parties_post1860[i] == 'rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf_idf_post1860 = make_TF_IDF(stemmed_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(tf_idf_post1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "similarity_within_dem = cos_sim[dem_idx,:][:,dem_idx]\n",
    "similarity_within_rep = cos_sim[rep_idx,:][:,rep_idx]\n",
    "similarity_between_parties = cos_sim[dem_idx,:][:,rep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(similarity_within_dem))\n",
    "print(np.mean(similarity_within_rep))\n",
    "print(np.mean(similarity_between_parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(similarity_within_dem)/np.mean(similarity_between_parties))\n",
    "print(np.mean(similarity_within_rep)/np.mean(similarity_between_parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "U, S, V = svds(tf_idf_post1860, k = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get low rank approximation of tf-idf matrix\n",
    "low_rank_approx = U.dot(np.diag(S)).dot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "low_rank_cos_sim = cosine_similarity(low_rank_approx)\n",
    "\n",
    "low_rank_similarity_within_dem = low_rank_cos_sim[dem_idx,:][:,dem_idx]\n",
    "low_rank_similarity_within_rep = low_rank_cos_sim[rep_idx,:][:,rep_idx]\n",
    "low_rank_similarity_between_parties = low_rank_cos_sim[dem_idx,:][:,rep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(low_rank_similarity_within_dem))\n",
    "print(np.mean(low_rank_similarity_within_rep))\n",
    "print(np.mean(low_rank_similarity_between_parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(low_rank_similarity_within_dem)/np.mean(low_rank_similarity_between_parties))\n",
    "print(np.mean(low_rank_similarity_within_rep)/np.mean(low_rank_similarity_between_parties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Interestingly, this analysis gives us that democrats are not much more similar to each other than they are to republicans, whereas republicans are much more similar to each other than they are to democrats. LSA here makes the distinction between parties somewhat clearer, as it provides more evidence of clustering of democrats, but less of clustering of republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "data_post1965 = processed_data.loc[processed_data.year >= 1965]\n",
    "data_post1965 = data_post1965.reset_index(drop=True)\n",
    "parties = [pres_party[i] for i in data_post1965.president]\n",
    "data_post1965 = data_post1965.assign(party=parties)\n",
    "\n",
    "stemmed_post1965, processed_post1965 = data_processing(data_post1965)\n",
    "\n",
    "stemmed_post1965 = custom_stopword_del(stemmed_post1965, our_stopwords)\n",
    "stemmed_post1965, processed_post1965 = remove_zerolen_strings(stemmed_post1965, processed_post1965)\n",
    "\n",
    "parties_post1965 = [i for i in processed_post1965.party]\n",
    "dem_idx = [i for i in range(len(parties_post1965)) if parties_post1965[i] == 'dem']\n",
    "rep_idx = [i for i in range(len(parties_post1965)) if parties_post1965[i] == 'rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity\n",
    "\n",
    "tf_idf_post1965 = make_TF_IDF(stemmed_post1965)\n",
    "\n",
    "cos_sim = cosine_similarity(tf_idf_post1965)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compare similarity within and between parties\n",
    "print( np.mean(cos_sim[dem_idx,:][:,dem_idx]) ) \n",
    "print( np.mean(cos_sim[rep_idx,:][:,rep_idx]) ) \n",
    "print( np.mean(cos_sim[dem_idx,:][:,rep_idx]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.mean(cos_sim[dem_idx,:][:,dem_idx])/np.mean(cos_sim[dem_idx,:][:,rep_idx]))\n",
    "print(np.mean(cos_sim[rep_idx,:][:,rep_idx])/np.mean(cos_sim[dem_idx,:][:,rep_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, S, V = svds(tf_idf_post1965, k = 200)\n",
    "\n",
    "low_rank_approx = U.dot(np.diag(S)).dot(V)\n",
    "\n",
    "low_rank_cos_sim = cosine_similarity(low_rank_approx)\n",
    "\n",
    "low_rank_similarity_within_dem = low_rank_cos_sim[dem_idx,:][:,dem_idx]\n",
    "low_rank_similarity_within_rep = low_rank_cos_sim[rep_idx,:][:,rep_idx]\n",
    "low_rank_similarity_between_parties = low_rank_cos_sim[dem_idx,:][:,rep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.mean(low_rank_similarity_within_dem)/np.mean(low_rank_similarity_between_parties))\n",
    "print(np.mean(low_rank_similarity_within_rep)/np.mean(low_rank_similarity_between_parties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# E Step\n",
    "def E_step(rho_i, B_i, count_matrix):\n",
    "    L =  np.log(rho_i) + count_matrix.dot(np.log(B_i.T))\n",
    "    z_hat = np.exp((L.T - logsumexp(L, axis=1)).T)\n",
    "    return z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# M Step\n",
    "def rho_update(z_hat, count_matrix):\n",
    "    D = np.shape(count_matrix)[0]\n",
    "    rho_i = np.sum(z_hat, axis = 0) / D\n",
    "    return rho_i\n",
    "\n",
    "def beta_update(z_hat, count_matrix, N_d):\n",
    "    lower_bound =  np.finfo('float').max**(-1)\n",
    "    B_i = (count_matrix.T.dot(z_hat) / np.sum(z_hat.T * N_d, axis=1)).T\n",
    "    B_i[B_i == 0.0] = lower_bound\n",
    "    return B_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate log-likelihood\n",
    "def MM_loglik(rho_i, B_i, count_matrix):\n",
    "    # Calculate log-likelihood of Multinomial Mixture Model\n",
    "    L =  np.log(rho_i) + count_matrix.dot(np.log(B_i.T))\n",
    "    L[L <= -500] = -500\n",
    "    L =  np.exp(L)\n",
    "    ll = np.sum(L, axis = 1)\n",
    "    if ll.min() == 0.0:\n",
    "        ll[ll==0.0] = np.finfo('float').max**(-1)\n",
    "    ll = np.sum(np.log(ll))\n",
    "    return(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Multinom_Mixt_EM(data, k, max_iters = 100, eps = 10^(-3)):\n",
    "    count_matrix = make_count(data)\n",
    "    vocab = get_vocab(data)\n",
    "    D = len(data)\n",
    "    n = len(vocab)\n",
    "    N_d = [len(x) for x in data]\n",
    "\n",
    "    # Initialise params\n",
    "    rho_i   = [1/k]*k\n",
    "    B_i     = np.random.dirichlet([1]*n, size=k)\n",
    "    loglik_seq = [MM_loglik(rho_i, B_i, count_matrix)]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # E step\n",
    "        z_hat   = E_step(rho_i, B_i, count_matrix)\n",
    "\n",
    "        # M step\n",
    "        rho_i   = rho_update(z_hat, count_matrix)\n",
    "        B_i     = beta_update(z_hat,count_matrix, N_d)\n",
    "        loglik_seq.append(MM_loglik(rho_i, B_i, count_matrix))\n",
    "\n",
    "        # Early stopping criterion\n",
    "        if (loglik_seq[len(loglik_seq) - 1] - loglik_seq[len(loglik_seq) - 2]) <= eps:\n",
    "            return [z_hat, rho_i, B_i, loglik_seq]\n",
    "\n",
    "    return [z_hat, rho_i, B_i, loglik_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "z_hat, rho_i, B_i, loglik_seq = Multinom_Mixt_EM(stemmed, k=3, max_iters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loglik_seq[20:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
